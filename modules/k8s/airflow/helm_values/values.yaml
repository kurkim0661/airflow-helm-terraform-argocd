# Helm common value https://github.com/airflow-helm/charts/blob/main/charts/airflow/values.yaml

serviceAccount:
  create: true
  name: airflow
  annotation: {}


###################################
## CONFIG | Kubernetes Ingress
###################################
ingress:
  ## if we should deploy Ingress resources
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/ingress.md
  enabled: false

# ingress:
#   enabled: true
#   web:
#     annotations:
#       nginx.ingress.kubernetes.io/force-ssl-redirect: 'true'

dags:
  gitSync:
    enabled: true
    repo: "ssh://git@github.com:kurkim0661/airflow-dags.git"
    subPath: "dags"
    rev: HEAD
    branch: "main"
    depth: 1
    sshKeySecret: airflow-ssh-secret
    resources:
      limits:
        cpu: "50m"
        memory: "128Mi"
      requests:
        cpu: "10m"
        memory: "50Mi"

extraSecrets:
  airflow-ssh-secret:
    type: 'Opaque'
    data: |
      gitSshKey: 'YOUR-SSH-PRIVATE-KEY-BASE64'

airflow:
  image:
    repository: apache/airflow
    tag: 2.5.3-python3.8
    pullPolicy: Always
    pullSecret: ""
    uid: 50000
    gid: 0
  executor: CeleryKubernetesExecutor
  # python3 -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)"
  fernetKey: BhuZsGEcmhs3tyxLUD7F54sjB1rJ0R3MK104Hv7DKLA=
  users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: admin
      lastName: admin
  connections:
    - id: my_postgres
      type: postgres
      description: my Postgres connection
      host: postgres.example.com
      port: 5432
      login: XXXXXXXX
      password: XXXXXXXX
      schema: my_database
      extra: |
        { 
          "sslmode": "allow" 
        }
  variables:
    - key: "environment"
      value: "dev"
  config:
    # Core
    AIRFLOW__CORE__DAGS_FOLDER: /usr/local/airflow/dags/src
    AIRFLOW__CORE__COLORED_CONSOLE_LOG: 'False'
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'False'
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"

    # Webserver
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'False'
    AIRFLOW__WEBSERVER__AUTHENTICATE: 'True'
    AIRFLOW__WEBSERVER__HIDE_PAUSED_DAGS_BY_DEFAULT: 'True'
    AIRFLOW__WEBSERVER__AUTH_BACKEND: airflow.contrib.auth.backends.google_auth
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'

    # Google
    AIRFLOW__GOOGLE__OAUTH_CALLBACK_ROUTE: /oauth2callback
    AIRFLOW__GOOGLE__DOMAIN: yourcompany.com

    # Scheduler
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"

    # Triggerer
    AIRFLOW__TRIGGERER__DEFAULT_CAPACITY: "1000"

    # Celery
    AIRFLOW__CELERY__WORKER_CONCURRENCY: "30"
  pools:
    - name: "pool_1"
      description: "pool_1"
      slots: 5
    - name: "pool_2"
      description: "pool_2"
      slots: 0
      policies:
        - name: "scale up at 7pm UTC"
          slots: 50
          recurrence: "0 19 * * *"
        - name: "scale down at 6pm UTC"
          slots: 10
          recurrence: "0 6 * * *"
  poolsUpdate: true
  clusterDomain: "cluster.local"
scheduler:
  resources:
    limits:
      cpu: "500m"
      memory: "1Gi"
    requests:
      cpu: "50m"
      memory: "128Mi"
  securityContext:
    fsGroup: 65534

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo

web:
  initialDelaySeconds: '240'
  resources:
    limits:
      cpu: "800m"
      memory: "2Gi"
    requests:
      cpu: "50m"
      memory: "512Mi"
  securityContext:
    fsGroup: 65534

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo

workers:
  replicas: 1
  resources:
    limits:
      cpu: "2000m"
      memory: "4Gi"
    requests:
      cpu: "1000m"
      memory: "1Gi"

  keda:
    enabled: true
    namespaceLabels: {}
    # How often KEDA polls the airflow DB to report new scale requests to the HPA
    pollingInterval: 5
    # How many seconds KEDA will wait before scaling to zero.
    # Note that HPA has a separate cooldown period for scale-downs
    cooldownPeriod: 30
    # Minimum number of workers created by keda
    minReplicaCount: 0
    # Maximum number of workers created by keda
    maxReplicaCount: 10

    # Specify HPA related options
    advanced:
      horizontalPodAutoscalerConfig:
        behavior:
          scaleDown:
            stabilizationWindowSeconds: 300
            policies:
              - type: Percent
                value: 100
                periodSeconds: 15
        # Query to use for KEDA autoscaling. Must return a single integer.

  persistence:
    # Enable persistent volumes
    enabled: true
    # Volume size for worker StatefulSet
    size: 1Gi
    # If using a custom storageClass, pass name ref to all statefulSets here
    storageClassName:
    # Execute init container to chown log directory.
    # This is currently only needed in kind, due to usage
    # of local-path provisioner.
    fixPermissions: false
    # Annotations to add to worker volumes
    annotations: {}
    # Detailed default security context for persistence for container level
    securityContexts:
      container: {}

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo-m02
  securityContext:
    fsGroup: 65534

###################################
## COMPONENT | Triggerer
###################################
triggerer:
  ## if the airflow triggerer should be deployed
  enabled: true

  ## the number of triggerer Pods to run
  replicas: 1

  ## resource requests/limits for the triggerer Pods
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#resourcerequirements-v1-core
  resources: {}
  persistence:
    # Enable persistent volumes
    enabled: true
    # Volume size for triggerer StatefulSet
    size: 10Gi
    # If using a custom storageClass, pass name ref to all statefulSets here
    storageClassName:
    # Execute init container to chown log directory.
    # This is currently only needed in kind, due to usage
    # of local-path provisioner.
    fixPermissions: false
    # Annotations to add to triggerer volumes
    annotations: {}
  ## maximum number of triggers each triggerer will run at once (sets `AIRFLOW__TRIGGERER__DEFAULT_CAPACITY`)
  # capacity: 1000
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo-m03

###################################
## COMPONENT | Flower
###################################
flower:
  ## if the airflow flower UI should be deployed
  enabled: true

  ## resource requests/limits for the flower Pod
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#resourcerequirements-v1-core
  resources: {}

  ## configs for the Service of the flower Pods
  service:
    type: ClusterIP
    ports:
    ports:
      - name: flower-ui
        port: "{{ .Values.ports.flowerUI }}"

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo-m03

###################################
## CONFIG | Airflow Logs
###################################
logs:
  persistence:
    # Enable persistent volume for storing logs
    enabled: false
    # Volume size for logs
    size: 10Gi
    # Annotations for the logs PVC
    annotations: {}
    # If using a custom storageClass, pass name here
    storageClassName:
    ## the name of an existing PVC to use
    existingClaim:

###################################
## DATABASE | PgBouncer
###################################
pgbouncer:
  ## if the pgbouncer Deployment is created
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/pgbouncer.md
  enabled: true
  replicas: 1
  # Max number of old replicasets to retain
  revisionHistoryLimit: ~
  # Command to use for PgBouncer(templated).
  command: ["pgbouncer", "-u", "nobody", "/etc/pgbouncer/pgbouncer.ini"]
  # Args to use for PgBouncer(templated).
  args: ~
  
  ## resource requests/limits for the pgbouncer Pods
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#resourcerequirements-v1-core
  resources: {}

  ## sets pgbouncer config: `auth_type`
  auth_type: md5
  auth_file: /etc/pgbouncer/users.txt


  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo-m03

###################################
## DATABASE | Embedded Postgres
###################################
postgresql:
  ## if the `stable/postgresql` chart is used
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/embedded-database.md
  ## [WARNING] the embedded Postgres is NOT SUITABLE for production deployments of Airflow
  ## [WARNING] consider using an external database with `externalDatabase.*`
  enabled: true

  ## configs for the PVC of postgresql
  persistence:
    enabled: true
    storageClass: ""
    size: 8Gi

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo-m03

###################################
## DATABASE | External Database
###################################
externalDatabase:
  ## the type of external database
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/external-database.md
  type: postgres

###################################
## DATABASE | Embedded Redis
###################################
redis:
  ## if the `stable/redis` chart is used
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/embedded-redis.md
  ## [WARNING] consider using an external database with `externalDatabase.*`
  enabled: true
  terminationGracePeriodSeconds: 600

  # Create ServiceAccount
  serviceAccount:
    # default value is true
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    automountServiceAccountToken: true
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the release name
    name: ~

    # Annotations to add to worker kubernetes service account.
    annotations: {}

  persistence:
    # Enable persistent volumes
    enabled: true
    # Volume size for worker StatefulSet
    size: 10Gi
    # If using a custom storageClass, pass name ref to all statefulSets here
    storageClassName:
    # Annotations to add to redis volumes
    annotations: {}

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - airflow-demo

###################################
## DATABASE | External Redis
###################################
externalRedis:
  ## the host of the external redis
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/external-redis.md
  host: localhost

# All ports used by chart
ports:
  flowerUI: 5555
  airflowUI: 8080
  workerLogs: 8793
  triggererLogs: 8794
  redisDB: 6379
  statsdIngest: 9125
  statsdScrape: 9102
  pgbouncer: 6543
  pgbouncerScrape: 9127

createUserJob:
  useHelmHooks: false
  applyCustomEnv: false
migrateDatabaseJob:
  useHelmHooks: false
  applyCustomEnv: false